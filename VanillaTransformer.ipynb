{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE as TokenizerBPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from torch.utils .data import Dataset, DataLoader, random_split\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6246a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "english",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "german",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "5166fb86-e5b8-49db-b99c-79bd400d9679",
       "rows": [
        [
         "0",
         "Two young, White males are outside near many bushes.",
         "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche."
        ],
        [
         "1",
         "Several men in hard hats are operating a giant pulley system.",
         "Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem."
        ],
        [
         "2",
         "A little girl climbing into a wooden playhouse.",
         "Ein kleines Mädchen klettert in ein Spielhaus aus Holz."
        ],
        [
         "3",
         "A man in a blue shirt is standing on a ladder cleaning a window.",
         "Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster."
        ],
        [
         "4",
         "Two men are at the stove preparing food.",
         "Zwei Männer stehen am Herd und bereiten Essen zu."
        ],
        [
         "5",
         "A man in green holds a guitar while the other man observes his shirt.",
         "Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht."
        ],
        [
         "6",
         "A man is smiling at a stuffed lion",
         "Ein Mann lächelt einen ausgestopften Löwen an."
        ],
        [
         "7",
         "A trendy girl talking on her cellphone while gliding slowly down the street.",
         "Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt."
        ],
        [
         "8",
         "A woman with a large purse is walking by a gate.",
         "Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei."
        ],
        [
         "9",
         "Boys dancing on poles in the middle of the night.",
         "Jungen tanzen mitten in der Nacht auf Pfosten."
        ],
        [
         "10",
         "A ballet class of five girls jumping in sequence.",
         "Eine Ballettklasse mit fünf Mädchen, die nacheinander springen."
        ],
        [
         "11",
         "Four guys three wearing hats one not are jumping at the top of a staircase.",
         "Vier Typen, von denen drei Hüte tragen und einer nicht, springen oben in einem Treppenhaus."
        ],
        [
         "12",
         "A black dog and a spotted dog are fighting",
         "Ein schwarzer Hund und ein gefleckter Hund kämpfen."
        ],
        [
         "13",
         "A man in a neon green and orange uniform is driving on a green tractor.",
         "Ein Mann in einer neongrünen und orangefarbenen Uniform fährt auf einem grünen Traktor."
        ],
        [
         "14",
         "Several women wait outside in a city.",
         "Mehrere Frauen warten in einer Stadt im Freien."
        ],
        [
         "15",
         "A lady in a black top with glasses is sprinkling powdered sugar on a bundt cake.",
         "Eine Frau mit schwarzem Oberteil und Brille streut Puderzucker auf einem Gugelhupf."
        ],
        [
         "16",
         "A little girl is sitting in front of a large painted rainbow.",
         "Ein kleines Mädchen sitzt vor einem großen gemalten Regenbogen."
        ],
        [
         "17",
         "A man lays on the bench to which a white dog is also tied.",
         "Ein Mann liegt auf der Bank, an die auch ein weißer Hund angebunden ist."
        ],
        [
         "18",
         "Five people are sitting in a circle with instruments.",
         "Fünf Personen sitzen mit Instrumenten im Kreis."
        ],
        [
         "19",
         "A bunch of elderly women play their clarinets together as they read off sheet music.",
         "Eine Gruppe älterer Frauen spielt zusammen Klarinette von Notenblättern."
        ],
        [
         "20",
         "A large structure has broken and is laying in a roadway.",
         "Ein großes Bauwerk ist kaputt gegangen und liegt auf einer Fahrbahn."
        ],
        [
         "21",
         "A large crowd of people stand outside in front of the entrance to a Metro station.",
         "Eine große Menschenmenge steht außen vor dem Eingang einer Metrostation."
        ],
        [
         "22",
         "A man getting a tattoo on his back.",
         "Ein Mann, der ein Tattoo auf seinem Rücken erhält."
        ],
        [
         "23",
         "Two children sit on a small seesaw in the sand.",
         "Zwei Kinder sitzen auf einer kleinen Wippe im Sand."
        ],
        [
         "24",
         "A man wearing a reflective vest and a hard hat holds a flag in the road",
         "Ein Mann, der eine reflektierende Weste und einen Schutzhelm trägt, hält eine Flagge in die Straße."
        ],
        [
         "25",
         "A person dressed in a blue coat is standing in on a busy sidewalk, studying painting of a street scene.",
         "Eine Person in einem blauen Mantel steht auf einem belebten Gehweg und betrachtet ein Gemälde einer Straßenszene."
        ],
        [
         "26",
         "A man in green pants walking down the road.",
         "Ein Mann in grünen Hosen läuft die Straße entlang."
        ],
        [
         "27",
         "The small child climbs on a red ropes on a playground.",
         "Das kleine Kind klettert an roten Seilen auf einem Spielplatz."
        ],
        [
         "28",
         "You know i am looking like Justin Bieber.",
         "Du weißt, dass ich aussehe wie Justin Bieber."
        ],
        [
         "29",
         "A young man in a black and yellow jacket is gazing at something and smiling.",
         "Ein junger Mann in einer schwarz-gelben Jacke blickt etwas an und lächelt."
        ],
        [
         "30",
         "A man standing at a urinal with a coffee cup.",
         "Ein Mann, der mit einer Tasse Kaffee an einem Urinal steht."
        ],
        [
         "31",
         "Five people walking with a multicolored sky in the background.",
         "Fünf gehende Personen mit einem mehrfarbigen Himmel im Hintergrund."
        ],
        [
         "32",
         "A old man having a beer alone.",
         "Ein alter Mann, der allein ein Bier trinkt."
        ],
        [
         "33",
         "A trained police dog sits next to his handler in front of the police van.",
         "Ein geschulter Polizeihund sitzt neben dem Hundeführer vor dem Polizeitransporter."
        ],
        [
         "34",
         "A person riding a bike on a snowy road.",
         "Eine Person fährt auf einer verschneiten Straße Fahrrad."
        ],
        [
         "35",
         "Five men, uniformly dressed in white shirts, tie and black slacks converse at the back of an open van.",
         "Fünf Männer, die alle weiße Hemden, Krawatten und schwarze Freizeithosen tragen, unterhalten sich hinter einem Lieferwagen."
        ],
        [
         "36",
         "A man with a backwards hat works on machinery.",
         "Ein Mann mit einem nach hinten gerichteten Hut arbeitet an Maschinen."
        ],
        [
         "37",
         "A black woman and a white man working in a factory setting packing jars with candles into boxes.",
         "Eine schwarze Frau und ein weißer Mann arbeiten in einer Fabrikumgebung und packen Gläser mit Kerzen in Kartons."
        ],
        [
         "38",
         "Asian man sweeping the walkway.",
         "Ein asiatischer Mann kehrt den Gehweg."
        ],
        [
         "39",
         "A man leans into a car to talk to the driver, as a man on a bicycle looks on.",
         "Ein Mann lehnt sich in ein Auto, um mit dem Fahrer zu reden, während ein Mann auf einem Fahrrad zusieht."
        ],
        [
         "40",
         "Two young toddlers outside on the grass.",
         "Zwei Kleinkinder im Freien auf dem Gras."
        ],
        [
         "41",
         "People are watching a person in a weird vehicle in a plaza.",
         "Leute sehen einer Person in einem seltsamen Fahrzeug auf einem Platz zu."
        ],
        [
         "42",
         "A man walks by a silver vehicle.",
         "Ein Mann geht an einem silbernen Fahrzeug vorbei."
        ],
        [
         "43",
         "A beautiful bride walking on a sidewalk with her new husband.",
         "Eine schöne Braut geht auf einem Gehweg mit ihrem neuen Ehemann."
        ],
        [
         "44",
         "A little boy playing GameCube at a McDonald's.",
         "Ein kleiner Junge spielt bei McDonald's GameCube."
        ],
        [
         "45",
         "A white dog shakes on the edge of a beach with an orange ball.",
         "Ein weißer Hund schüttelt sich am Rande eines Strands mit einem orangefarbenen Ball."
        ],
        [
         "46",
         "A group of people having a barbecue at a park.",
         "Eine Gruppe von Personen, die im Park grillen."
        ],
        [
         "47",
         "A man in sunglasses puts his arm around a woman in a black and white blouse.",
         "Ein Mann mit Sonnenbrille legt seinen Arm um eine Frau in einer schwarz-weißen Bluse."
        ],
        [
         "48",
         "A man with a balloon hat and people eating outdoors at picnic tables.",
         "Ein Mann mit einem Luftballonhut und Leute, die im Freien an Picknicktischen essen."
        ],
        [
         "49",
         "A boy jump kicking over three kids kicking wood during a tae kwon do competition.",
         "Ein Junge, der während eines Taekwondo-Wettbewerbs einen Sprungtritt über drei Kinder macht und dabei auf Holz tritt."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 29000
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two young, White males are outside near many b...</td>\n",
       "      <td>Zwei junge weiße Männer sind im Freien in der ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several men in hard hats are operating a giant...</td>\n",
       "      <td>Mehrere Männer mit Schutzhelmen bedienen ein A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A little girl climbing into a wooden playhouse.</td>\n",
       "      <td>Ein kleines Mädchen klettert in ein Spielhaus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man in a blue shirt is standing on a ladder ...</td>\n",
       "      <td>Ein Mann in einem blauen Hemd steht auf einer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two men are at the stove preparing food.</td>\n",
       "      <td>Zwei Männer stehen am Herd und bereiten Essen zu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28995</th>\n",
       "      <td>A woman behind a scrolled wall is writing</td>\n",
       "      <td>Eine Frau schreibt hinter einer verschnörkelte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28996</th>\n",
       "      <td>A rock climber practices on a rock climbing wall.</td>\n",
       "      <td>Ein Bergsteiger übt an einer Kletterwand.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28997</th>\n",
       "      <td>Two male construction workers are working on a...</td>\n",
       "      <td>Zwei Bauarbeiter arbeiten auf einer Straße vor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28998</th>\n",
       "      <td>An elderly man sits outside a storefront accom...</td>\n",
       "      <td>Ein älterer Mann sitzt mit einem Jungen mit ei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28999</th>\n",
       "      <td>A man in shorts and a Hawaiian shirt leans ove...</td>\n",
       "      <td>Ein Mann in Shorts und Hawaiihemd lehnt sich ü...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english  \\\n",
       "0      Two young, White males are outside near many b...   \n",
       "1      Several men in hard hats are operating a giant...   \n",
       "2        A little girl climbing into a wooden playhouse.   \n",
       "3      A man in a blue shirt is standing on a ladder ...   \n",
       "4               Two men are at the stove preparing food.   \n",
       "...                                                  ...   \n",
       "28995          A woman behind a scrolled wall is writing   \n",
       "28996  A rock climber practices on a rock climbing wall.   \n",
       "28997  Two male construction workers are working on a...   \n",
       "28998  An elderly man sits outside a storefront accom...   \n",
       "28999  A man in shorts and a Hawaiian shirt leans ove...   \n",
       "\n",
       "                                                  german  \n",
       "0      Zwei junge weiße Männer sind im Freien in der ...  \n",
       "1      Mehrere Männer mit Schutzhelmen bedienen ein A...  \n",
       "2      Ein kleines Mädchen klettert in ein Spielhaus ...  \n",
       "3      Ein Mann in einem blauen Hemd steht auf einer ...  \n",
       "4      Zwei Männer stehen am Herd und bereiten Essen zu.  \n",
       "...                                                  ...  \n",
       "28995  Eine Frau schreibt hinter einer verschnörkelte...  \n",
       "28996          Ein Bergsteiger übt an einer Kletterwand.  \n",
       "28997  Zwei Bauarbeiter arbeiten auf einer Straße vor...  \n",
       "28998  Ein älterer Mann sitzt mit einem Jungen mit ei...  \n",
       "28999  Ein Mann in Shorts und Hawaiihemd lehnt sich ü...  \n",
       "\n",
       "[29000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = get_config()\n",
    "train_data=\"C:/Users/pc/Downloads/archive/translation_train.csv\"\n",
    "test_data=\"C:/Users/pc/Downloads/archive/translation_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_data)\n",
    "test_df = pd.read_csv(test_data)\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "train_df = train_df[train_df['english'].str.strip() != '']\n",
    "test_df = test_df[test_df['english'].str.strip() != '']\n",
    "\n",
    "train_df = train_df[train_df['german'].str.strip() != '']\n",
    "test_df = test_df[test_df['german'].str.strip() != '']\n",
    "\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bcf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_combined_text = train_df['english'].str.cat(sep=' ')\n",
    "ge_combined_text = train_df['german'].str.cat(sep=' ')\n",
    "combined_text = en_combined_text + ' ' + ge_combined_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37feb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # ! TOO Large, need to be Quantize\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds[self.src_lang])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.ds[self.src_lang][idx]\n",
    "        tgt_text = self.ds[self.tgt_lang][idx]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # ! Need to Quantize\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b956fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds.iterrows():\n",
    "        yield item[1][lang]\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Use a different variable name to avoid the conflict\n",
    "        tokenizer_instance = Tokenizer(TokenizerBPE(unk_token='[UNK]'))\n",
    "        tokenizer_instance.pre_tokenizer = Whitespace()\n",
    "        trainer = BpeTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n",
    "        tokenizer_instance.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer_instance.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer_instance = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer_instance\n",
    "\n",
    "def get_ds(config, ds_raw, split_ratio=0.9):\n",
    "    \n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Combine the lists into a list of tuples\n",
    "    combined = list(zip(ds_raw['english'], ds_raw['german']))\n",
    "\n",
    "    # Split the combined list into training and validation sets\n",
    "\n",
    "    train_ds_size = int(len(combined) * split_ratio)\n",
    "    val_ds_size = len(combined) - train_ds_size\n",
    "    train_combined, val_combined = random_split(combined, [train_ds_size, val_ds_size])\n",
    "\n",
    "\n",
    "    # Unzip the combined lists back into separate lists\n",
    "    train_en, train_fr = zip(*train_combined)\n",
    "    val_en, val_fr = zip(*val_combined)\n",
    "\n",
    "    # Combine the splits into new dictionaries\n",
    "    train_set = {'english': list(train_en), 'german': list(train_fr)}\n",
    "    val_set = {'english': list(val_en), 'german': list(val_fr)}\n",
    "\n",
    "\n",
    "    train_ds = BilingualDataset(train_set, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_set, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for idx in range( len( ds_raw[config['lang_src']]) ):\n",
    "        src_ids = tokenizer_src.encode(ds_raw['english'][idx]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(ds_raw['german'][idx]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af695bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(\n",
    "                0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    \n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate \n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "def train_model(config, ds_raw):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config, ds_raw, config['split_ratio'])\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8e6baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce RTX 4070 SUPER\n",
      "Device memory: 11.99365234375 GB\n",
      "Max length of source sentence: 41\n",
      "Max length of target sentence: 46\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|██████████| 3263/3263 [02:25<00:00, 22.39it/s, loss=4.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A man is standing in front of a building holding heart shaped balloons and a woman is crossing the street.\n",
      "    TARGET: Ein Mann steht vor einem Gebäude und hält herzförmige Luftballons, und eine Frau überquert die Straße.\n",
      " PREDICTED: Ein Mann steht vor einem Tisch , der eine Frau hält , während eine Frau in der Hand .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A person is walking on the night street under neon lights.\n",
      "    TARGET: Eine Person schlendert nachts im Neonlicht die Straße entlang.\n",
      " PREDICTED: Eine Person geht auf der Straße entlang , die die Straße .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "d:\\D-Program Files\\Python\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `BLEUScore` from `torchmetrics` was deprecated and will be removed in 2.0. Import `BLEUScore` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "Processing Epoch 01: 100%|██████████| 3263/3263 [02:15<00:00, 24.08it/s, loss=4.326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A woman and a baby eating (having a picnic).\n",
      "    TARGET: Eine Frau und ein Kleinkind essen (picknicken).\n",
      " PREDICTED: Eine Frau und ein Baby posieren in einem Waschsalon .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A boy sits on the grass near a geometric sculpture.\n",
      "    TARGET: Ein Junge sitzt auf dem Rasen in der Nähe von einer geometrischen Skulptur.\n",
      " PREDICTED: Ein Junge sitzt auf dem Gras und macht eine Pause in der Nähe eines Hauses .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100%|██████████| 3263/3263 [02:25<00:00, 22.41it/s, loss=3.226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A cello player and a violinist getting ready for their performance in an elegant room.\n",
      "    TARGET: Ein Cellospieler und ein Geiger bereiten sich in einem eleganten Raum auf ihren Auftritt vor.\n",
      " PREDICTED: Ein paar männliche und ein paar Mädchen bereiten sich in einem Raum für ihre Kleidung auf .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A person walks among large, white geometric shaped architecture.\n",
      "    TARGET: Eine Person geht unter großen geometrischen Bauwerken.\n",
      " PREDICTED: Eine Person geht mit großen weißen , weißen Obst .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100%|██████████| 3263/3263 [02:25<00:00, 22.50it/s, loss=3.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two little boys play in the water left behind by the sprinklers.\n",
      "    TARGET: Zwei kleine Jungen spielen im Wasser aus dem Wassersprenger.\n",
      " PREDICTED: Zwei kleine Jungen spielen in der linken linken linken linken linken linken Seite .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Front stroke swimming race roped off lap areas.\n",
      "    TARGET: @@\n",
      " PREDICTED: Bei einem Rennen , das ein Rennen mit dem Schoß ist .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100%|██████████| 3263/3263 [02:22<00:00, 22.86it/s, loss=2.170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Three teenagers dancing one evening on the street.\n",
      "    TARGET: Drei Teenager tanzen an einem Abend auf der Straße.\n",
      " PREDICTED: Drei Teenager tanzen an der Straße .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A construction crew working at several spots on the road.\n",
      "    TARGET: Eine Bauarbeitermannschaft, die an mehreren Stellen auf der Straße arbeitet.\n",
      " PREDICTED: Ein Bauarbeiter arbeitet an mehreren Computer s auf der Straße .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100%|██████████| 3263/3263 [02:15<00:00, 24.09it/s, loss=2.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A brown-haired woman is holding a baby boy wearing a shirt with Reindeer on it and a chef's hat.\n",
      "    TARGET: Eine braunhaarige Frau hält einen kleinen Jungen, der ein Oberteil mit Rentiermotiv und eine Kochmütze trägt.\n",
      " PREDICTED: Eine braunhaarige Frau hält ein Baby , das ein Baby trägt , das mit einem Hut mit einem Hut und einem Koch trägt .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A man in shorts walks a big brown dog and a woman pushing a baby carriage walks right behind him.\n",
      "    TARGET: Ein Mann in Shorts führt einen großen braunen Hund spazieren und eine Frau schiebt einen Kinderwagen direkt hinter ihm.\n",
      " PREDICTED: Ein Mann in kurzen Hosen geht einen großen Hund entlang , der hinter ihm eine Frau schiebt , einen Kinderwagen schiebt .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100%|██████████| 3263/3263 [02:14<00:00, 24.21it/s, loss=2.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A snowboarder is performing a stunt in mostly dark.\n",
      "    TARGET: Ein Snowboarder vollführt bei schummerigem Licht ein Kunststück.\n",
      " PREDICTED: Ein Snowboarder macht einen Trick in einem Kaufhaus .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Five little girls posing in matching dance outfits.\n",
      "    TARGET: Fünf kleine Mädchen posieren in einheitlichen Tanzoutfits.\n",
      " PREDICTED: Fünf kleine Mädchen in passenden Kleidern posieren .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100%|██████████| 3263/3263 [02:15<00:00, 24.14it/s, loss=2.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A man is engulfed in flames while two movie crewmen supervise.\n",
      "    TARGET: Ein Mann ist in Flammen gehüllt und wird von zwei Kollegen der Filmcrew überwacht.\n",
      " PREDICTED: Ein Mann ist dabei , zwei K ü schaft s in der Ferne zu machen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A seated boy plays the accordion.\n",
      "    TARGET: Ein sitzender Junge spielt Akkordeon.\n",
      " PREDICTED: Ein sitzender Junge spielt Akkordeon .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100%|██████████| 3263/3263 [02:18<00:00, 23.59it/s, loss=2.211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Several bikers on a stone road, with spectators watching.\n",
      "    TARGET: Mehrere Radfahrer fahren auf einer Steinstraße mit Zuschauern.\n",
      " PREDICTED: Mehrere Biker auf einer gepflasterten Straße , während Zuschauer zusehen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: This man in the yellow shirt is adjusting blue bicycle for a young boy.\n",
      "    TARGET: Der Mann in dem gelben Oberteil stellt ein blaues Fahrrad für einen kleinen Jungen ein.\n",
      " PREDICTED: Der Mann in gelbem Hemd schneidet sein blaues Fahrrad für einen kleinen Jungen .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100%|██████████| 3263/3263 [02:17<00:00, 23.77it/s, loss=2.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two men during a football game\n",
      "    TARGET: Zwei Männer während eines Fußballspiels\n",
      " PREDICTED: Zwei Männer bei einem Football - Spiel .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A guy in his scuba gear is having a conversation with other men.\n",
      "    TARGET: Ein Mann in Tauchausrüstung unterhält sich mit anderen Männern.\n",
      " PREDICTED: Ein Mann in seinem Taucher ausrüstung unterhält sich mit anderen Männern .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100%|██████████| 3263/3263 [02:15<00:00, 24.02it/s, loss=1.958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: An elderly woman wearing a bathing suit is holding a picket sign while participating in a demonstration about immigration.\n",
      "    TARGET: Eine ältere Frau im Badeanzug nimmt an einer Demonstration zum Thema Immigration teil und hält ein Plakat.\n",
      " PREDICTED: Eine ältere Frau in Badeanzug hält ein Schild mit einem Plakat , während sie an einem Plakat arbeitet .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A group of children in brown outfits are performing.\n",
      "    TARGET: Eine Gruppe von Kindern in brauner Kleidung führt etwas vor.\n",
      " PREDICTED: Eine Gruppe von Kindern in braunen Outfits führt etwas vor .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100%|██████████| 3263/3263 [02:31<00:00, 21.48it/s, loss=1.856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Three women and one man are having a friendly conversation in an office.\n",
      "    TARGET: Drei Frauen und ein Mann unterhalten sich freundlich in einem Büro.\n",
      " PREDICTED: Drei Frauen und ein Mann essen eine freund schaft liche Unterhaltung .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A boy jumps into a pool.\n",
      "    TARGET: Ein Junge springt in einen Pool.\n",
      " PREDICTED: Ein Junge springt in ein Schwimmbecken .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100%|██████████| 3263/3263 [02:21<00:00, 23.09it/s, loss=1.925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: People gather for a farmers market day, shopping for groceries and clothing on a sunny day.\n",
      "    TARGET: Personen versammeln sich auf einem Wochenmarkt, um Lebensmittel und Kleidung an einem sonnigen Tag zu kaufen.\n",
      " PREDICTED: Menschen versammeln sich zum Grillen , zum Essen und west liche Kleidung an einem sonnigen Tag .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Military personnel learning how to shoot their rifles.\n",
      "    TARGET: Soldaten lernen mit ihren Gewehren zu schießen.\n",
      " PREDICTED: Militärangehörige , der versucht , das Spiel des Waffen zu schießen .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100%|██████████| 3263/3263 [02:19<00:00, 23.47it/s, loss=1.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two street artists are performing on steps for a group of spectators.\n",
      "    TARGET: Zwei Straßenkünstler geben auf einer Treppe eine Vorstellung für Zuschauergruppe.\n",
      " PREDICTED: Zwei Straßenkünstler führen auf Stufen für eine Gruppe von Zuschauern auf .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A bicyclist in blue parked inside of an all wood building.\n",
      "    TARGET: Ein blau gekleideter Fahrradfahrer parkt in einem Holzhaus.\n",
      " PREDICTED: Ein Radfahrer in Blau parkt in einem noch Holz raum .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100%|██████████| 3263/3263 [02:20<00:00, 23.27it/s, loss=1.730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A young girl in a pink shirt creates a painting on paper.\n",
      "    TARGET: Ein kleines Mädchen in einem rosa Oberteil malt auf Papier.\n",
      " PREDICTED: Ein junges Mädchen in einem rosafarbenen Oberteil malt ein Papier auf Papier .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A crowd watching a man in white pants using an axe.\n",
      "    TARGET: Eine Menschenmenge schaut einem Mann in weißen Hosen und mit einer Axt zu.\n",
      " PREDICTED: Eine Menschenmenge beobachtet einen Mann in weißer Hose , der mit einer Axt benutzt .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100%|██████████| 3263/3263 [02:20<00:00, 23.26it/s, loss=1.835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two girls in matching pink and white dresses and two smaller boys in matching black and white shirts.\n",
      "    TARGET: Zwei Mädchen in zueinander passenden rosafarbenen und weißen Kleidern und zwei kleinere Jungen in zueinander passenden schwarz-weißen Hemden.\n",
      " PREDICTED: Zwei Mädchen in farblich passenden pinkfarbenen und weißen Kleidern und zwei kleine Jungen in schwarzen Hemden und weißen Hemden .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 3 dark-skinned males, two shirtless, are on a grassy area with trees scattered around them.\n",
      "    TARGET: Drei dunkelhäutige Männer, von denen zwei kein Hemd anhaben, befinden sich auf einer Grasfläche mit vereinzelten Bäumen um sie herum.\n",
      " PREDICTED: Drei dunkelhäutige Männer in schwarzen Hemden , zwei mit freiem Oberkörper , sind auf einer Grasfläche .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100%|██████████| 3263/3263 [02:21<00:00, 23.12it/s, loss=1.833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Two men speak closely at a party.\n",
      "    TARGET: Zwei Männer reden eng beieinander auf einer Party.\n",
      " PREDICTED: Zwei Männer sprechen bei einer Feier miteinander .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A man is looking on as another man attempts to climb a small boulder with his dirt bike.\n",
      "    TARGET: Ein Mann sieht zu, wie ein anderer Mann versucht, mit seinem Dirtbike einen kleinen Felsblock hinaufzuklettern.\n",
      " PREDICTED: Ein Mann schaut zu , wie ein anderer Mann versucht , mit seinem Geländemotorrad einen kleinen Rad hochzuklettern .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100%|██████████| 3263/3263 [02:26<00:00, 22.34it/s, loss=1.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Crowd watching airplane and helicopter in the sky.\n",
      "    TARGET: Eine Menschenmenge beobachten ein Flugzeug und einen Hubschrauber am Himmel.\n",
      " PREDICTED: Eine Menschenmenge beobachtet im Himmel und einem Hubschrauber .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A brown-haired girl stoops to pick up a small white puppy on an empty street.\n",
      "    TARGET: Ein braunhaariges Mädchen bückt sich, um auf einer leeren Straße einen kleinen weißen Hund hochzuheben.\n",
      " PREDICTED: Ein braunhaariges Mädchen geht auf einer leeren Straße nach einem kleinen weißen Welpen spazieren .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100%|██████████| 3263/3263 [02:16<00:00, 23.99it/s, loss=1.690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Several military men surrounding women who are sitting on the ground with flags.\n",
      "    TARGET: Mehrere männliche Soldaten kreisen Frauen mit Fahnen ein, die auf dem Boden sitzen.\n",
      " PREDICTED: Mehrere Soldaten stehen um eine sitzende Frau , die auf dem Boden sitzen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A woman in a black dress holding sunglasses.\n",
      "    TARGET: Eine Frau im schwarzen Kleis hält eine Sonnenbrille in der Hand.\n",
      " PREDICTED: Eine Frau in einem schwarzen Kleid hält eine Sonnenbrille .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100%|██████████| 3263/3263 [02:16<00:00, 23.82it/s, loss=1.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Hands with painted fingernails unscrewing nail polish.\n",
      "    TARGET: Hände mit lackierten Fingernägeln schrauben Nagellack auf.\n",
      " PREDICTED: Ein Rudel w in der Hocke , der strahlend den Stock durch den Schritt ist .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A guy in a hard hat working on some machines.\n",
      "    TARGET: Ein Mann mit Schutzhelm arbeitet an Maschinen.\n",
      " PREDICTED: Ein Mann mit Schutzhelm arbeitet an Maschinen .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 20:  11%|█         | 359/3263 [00:15<02:03, 23.52it/s, loss=1.521]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 154\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config, ds_raw)\u001b[0m\n\u001b[0;32m    151\u001b[0m decoder_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, 1, seq_len, seq_len)\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Run the tensors through the encoder, decoder and the projection layer\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m    155\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(encoder_output, encoder_mask, decoder_input, decoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m    156\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 214\u001b[0m, in \u001b[0;36mTransformer.encode\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m    212\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(src)\n\u001b[0;32m    213\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pos(src)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\D-Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\D-Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 158\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 158\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32md:\\D-Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\D-Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 146\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x, src_mask)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[0;32m    145\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_block(x, x, x, src_mask))\n\u001b[1;32m--> 146\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_block\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32md:\\D-Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\D-Program Files\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(config, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea581357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39732c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
